{
  "title": "Can Web Search Engines Index Wikis",
  "story": [
    {
      "type": "html",
      "text": "Conclusion: yes, Web search engines can index wikis. [[Google Loves Wiki]].",
      "id": "002c3fe3a570f85ed3ecec99f3e5aa25"
    },
    {
      "type": "html",
      "text": "\nTherefore, a HTTP search engine is a useful tool to provide increased search capability for any wiki.",
      "id": "f39fece7bfd9a57a1e879a0032c4060d"
    },
    {
      "type": "html",
      "text": "However, the [[Robots.txt]] file should be properly configured.",
      "id": "a23b06a31f930a1aef546ffeccca44a1"
    },
    {
      "type": "html",
      "text": "\nAnd the appropriate META tags should be added to \"[[Edit Page]]\" and \"[[Edit Copy]]\" to exclude all robots from indexing obsolete text.",
      "id": "c86cd61027d92f509d6633c5efa003c2"
    },
    {
      "type": "html",
      "text": "\nRobots.txt can be configured to allow a <i>local search engine</i> to index the site while excluding <i>public Internet search engines</i>, something like this:",
      "id": "1686276d5a8ab55250837f5d9f19781c"
    },
    {
      "type": "html",
      "text": "\nTo allow a single robot",
      "id": "2961ee93d6072fe93b5d889ef34db37d"
    },
    {
      "type": "html",
      "text": "User-agent:<br>Friendly<b></b>Web<b></b>Crawler",
      "id": "ec01470c8d5217140131770f04374e82"
    },
    {
      "type": "code",
      "text": " Disallow:",
      "id": "93a024f7a19cf646ece94ce2d94dfc12"
    },
    {
      "type": "html",
      "text": "User-agent:<br>*",
      "id": "c9f9f2b625381360e8b3820b56fcbcf5"
    },
    {
      "type": "html",
      "text": "Disallow:<br>/",
      "id": "b45b47c01085511254c00d52d270bbdb"
    },
    {
      "type": "html",
      "text": "\nSee [[Robots Dot Txt]] for a link to the file format spec.",
      "id": "a4918317fd14af925193a5c8b0a010fc"
    },
    {
      "type": "html",
      "text": "<hr>",
      "id": "f4af353307c64085ce8ee2c13e14248d"
    },
    {
      "type": "html",
      "text": "<b>Discussion</b>",
      "id": "2c37381c5ab58c6c6c656bb7e6cead15"
    },
    {
      "type": "html",
      "text": "<i>Any HTTP search engine can index any wiki. It is just a matter of combination of configuration, URL space and page content (META tags) whether it is possible. There are no inherent technical limits.</i>  (copied from [[Wiki Navigation Pattern]])",
      "id": "0de3620ccd14f3383add07702573d519"
    },
    {
      "type": "html",
      "text": "\nNote the following:",
      "id": "a02a909bb06886db9beaebdda5b4dd15"
    },
    {
      "type": "html",
      "text": " Some wikis use robots.txt to avoid being indexed.",
      "id": "3305c07a211501de48fdd61873804090"
    },
    {
      "type": "html",
      "text": " As for any other Web site, the searchbot increases traffic on the wiki, but, hopefully, the searchbot is well behaved and the increased traffic is negligible.",
      "id": "3b79540196906619c4e3991adba77238"
    },
    {
      "type": "html",
      "text": " As for any other Web site, the index is not continually updated, and thus not always up to date.",
      "id": "091d46f3817a435de8a3b8aa9c2c24c1"
    },
    {
      "type": "html",
      "text": "\nIf the term \"caching\" of wiki pages is used, my belief is that the term is used to describe a sort of virtual caching that occurs because the searchbot indexes a (dynamic) Web page that \"disappears\" between \"instantiations\".  The index is all that remains between instantiations of the dynamic page.",
      "id": "90c5724f94a27d4cb439b69988278c38"
    },
    {
      "type": "html",
      "text": "\nI don't know if there is a way for a wiki (or other Web site) to force a searchbot to update its index.",
      "id": "03afbfbc154a31bd0404eb7e84ba621d"
    },
    {
      "type": "html",
      "text": "<hr>",
      "id": "f4af353307c64085ce8ee2c13e14248d"
    },
    {
      "type": "html",
      "text": "<b>References</b>",
      "id": "e34cac796c51eb44315e07e0cd5aab63"
    },
    {
      "type": "html",
      "text": "\nA sample robots.txt (for the Portland Pattern Repository):",
      "id": "517b73c68c356de508a72bdfd98511e1"
    },
    {
      "type": "code",
      "text": " # Prevent all robots from getting lost in my databases\n User-agent: *\n Disallow: /cgi/\n Disallow: /cgi-bin/",
      "id": "6f586732df91d1c8098677293bf8c125"
    },
    {
      "type": "html",
      "text": "[http://usemod.com/cgi-bin/mb.pl?RobotsDotTxt usemod.com] :<br>A useful link to learn more about preventing a wiki site from being indexed, with \"sublinks\" worth following. ",
      "id": "645246c7b6a890e61f6454d138857dac"
    },
    {
      "type": "html",
      "text": "<hr>",
      "id": "f4af353307c64085ce8ee2c13e14248d"
    },
    {
      "type": "html",
      "text": "<b>Comments</b>",
      "id": "e52fad802d82b9490f65faa3323b671c"
    },
    {
      "type": "html",
      "text": "\nSure, why not? [[Meatball Wiki]] is indexed by Google. Some searches are bizarre. We got one trolling for kiddie porn once. Recently, Cliff decided to exclude bots from hitting the edit pages, but they are greatly encouraged to index the whole site there. On the other hand, Wiki is not indexed, nor will it likely ever be directly, although it is through the mirrors that aren't blocked. -- [[Sunir Shah]]",
      "id": "b86cf4658d68968651a4c51e966ba3a1"
    },
    {
      "type": "html",
      "text": "<hr>",
      "id": "f4af353307c64085ce8ee2c13e14248d"
    },
    {
      "type": "html",
      "text": "\nI wrote up a document about how to give the wiki pages some \"fake\" URLs that end in .html.  This doc is at [http://www.riceball.com/wikiwiki/MakingWikiWorkWithSearchEngines.html www.riceball.com] -- [[John Kawakami]]",
      "id": "c0928e47a3f8ac31e668f75319374b11"
    },
    {
      "type": "html",
      "text": "<hr>",
      "id": "f4af353307c64085ce8ee2c13e14248d"
    },
    {
      "type": "html",
      "text": "\nAug 6, 2004: Google no longer indexes the [[Edit Page]] or [[Edit Copy]] links. The wiki software now emits",
      "id": "8b75416eaefec6b76910c9c3052929a5"
    },
    {
      "type": "code",
      "text": "    <META NAME=\"ROBOTS\" CONTENT=\"NOINDEX, NOFOLLOW\">",
      "id": "2e35d6d6cd9744ba97cbd1650d4c038a"
    },
    {
      "type": "html",
      "text": "in the HTML header of the Edit page and the Edit Copy page. Google, like all properly-written robots, ignore such pages. See [[Wiki Spam]] for details.",
      "id": "8bf59819f93003ca8170c3d0eb616585"
    },
    {
      "type": "html",
      "text": "\nFeb 2. 2005: But, using the above implementation, the spider must still access the [[Edit Page]] or [[Edit Copy]] links to then find out they aren't supposed to be indexed.  Now google and blogs are implementing a rel=\"nofollow\" tag in links.  This could be added to the [[Edit Page]] button on each page to give robots a heads-up to not even bother trying to access the [[Edit Page]].  What is still unclear is whether the rel=\"nofollow\" tag really means \"do not follow this link\" to robots, or whether it means \"follow this link, but don't give it any credit in your page ranking scheme\".",
      "id": "d186f4eb01eb83b8374cccadbdf82a48"
    },
    {
      "type": "html",
      "text": "<hr>",
      "id": "f4af353307c64085ce8ee2c13e14248d"
    },
    {
      "type": "html",
      "text": "---\nMay 02, 2005: Meta Tag Tutorial Available \n[http://www.rahsoftware.com/tutorials/meta_tag_tutorial.aspx www.rahsoftware.com]",
      "id": "1185ab76e3dd67a8b0025ec70988e90e"
    },
    {
      "type": "html",
      "text": "\n[[Category Wiki Engine Review]]",
      "id": "161795e5e1cfed737326519554e5c46a"
    },
    {
      "type": "html",
      "text": "See original on  [http://c2.com/cgi/wiki?CanWebSearchEnginesIndexWikis c2.com]",
      "id": "d6fbfbf7394309b9a73d81ba3a62f616"
    }
  ],
  "journal": [
    {
      "date": 1310499743000,
      "id": "25d71256f24c1347653f1aa7a9576bc8",
      "type": "create",
      "item": {
        "title": "Can Web Search Engines Index Wikis",
        "story": [
          {
            "type": "html",
            "text": "Conclusion: yes, Web search engines can index wikis. [[Google Loves Wiki]].",
            "id": "002c3fe3a570f85ed3ecec99f3e5aa25"
          },
          {
            "type": "html",
            "text": "\nTherefore, a HTTP search engine is a useful tool to provide increased search capability for any wiki.",
            "id": "f39fece7bfd9a57a1e879a0032c4060d"
          },
          {
            "type": "html",
            "text": "\nHowever, the Robots.txt file should be properly configured.",
            "id": "a23b06a31f930a1aef546ffeccca44a1"
          },
          {
            "type": "html",
            "text": "\nAnd the appropriate META tags should be added to \"[[Edit Page]]\" and \"[[Edit Copy]]\" to exclude all robots from indexing obsolete text.",
            "id": "c86cd61027d92f509d6633c5efa003c2"
          },
          {
            "type": "html",
            "text": "\nRobots.txt can be configured to allow a <i>local search engine</i> to index the site while excluding <i>public Internet search engines</i>, something like this:",
            "id": "1686276d5a8ab55250837f5d9f19781c"
          },
          {
            "type": "html",
            "text": "\nTo allow a single robot",
            "id": "2961ee93d6072fe93b5d889ef34db37d"
          },
          {
            "type": "html",
            "text": "User-agent:<br>Friendly<b></b>Web<b></b>Crawler",
            "id": "ec01470c8d5217140131770f04374e82"
          },
          {
            "type": "code",
            "text": " Disallow:",
            "id": "93a024f7a19cf646ece94ce2d94dfc12"
          },
          {
            "type": "html",
            "text": "User-agent:<br>*",
            "id": "c9f9f2b625381360e8b3820b56fcbcf5"
          },
          {
            "type": "html",
            "text": "Disallow:<br>/",
            "id": "b45b47c01085511254c00d52d270bbdb"
          },
          {
            "type": "html",
            "text": "\nSee [[Robots Dot Txt]] for a link to the file format spec.",
            "id": "a4918317fd14af925193a5c8b0a010fc"
          },
          {
            "type": "html",
            "text": "<hr>",
            "id": "f4af353307c64085ce8ee2c13e14248d"
          },
          {
            "type": "html",
            "text": "<b>Discussion</b>",
            "id": "2c37381c5ab58c6c6c656bb7e6cead15"
          },
          {
            "type": "html",
            "text": "<i>Any HTTP search engine can index any wiki. It is just a matter of combination of configuration, URL space and page content (META tags) whether it is possible. There are no inherent technical limits.</i>  (copied from [[Wiki Navigation Pattern]])",
            "id": "0de3620ccd14f3383add07702573d519"
          },
          {
            "type": "html",
            "text": "\nNote the following:",
            "id": "a02a909bb06886db9beaebdda5b4dd15"
          },
          {
            "type": "html",
            "text": " Some wikis use robots.txt to avoid being indexed.",
            "id": "3305c07a211501de48fdd61873804090"
          },
          {
            "type": "html",
            "text": " As for any other Web site, the searchbot increases traffic on the wiki, but, hopefully, the searchbot is well behaved and the increased traffic is negligible.",
            "id": "3b79540196906619c4e3991adba77238"
          },
          {
            "type": "html",
            "text": " As for any other Web site, the index is not continually updated, and thus not always up to date.",
            "id": "091d46f3817a435de8a3b8aa9c2c24c1"
          },
          {
            "type": "html",
            "text": "\nIf the term \"caching\" of wiki pages is used, my belief is that the term is used to describe a sort of virtual caching that occurs because the searchbot indexes a (dynamic) Web page that \"disappears\" between \"instantiations\".  The index is all that remains between instantiations of the dynamic page.",
            "id": "90c5724f94a27d4cb439b69988278c38"
          },
          {
            "type": "html",
            "text": "\nI don't know if there is a way for a wiki (or other Web site) to force a searchbot to update its index.",
            "id": "03afbfbc154a31bd0404eb7e84ba621d"
          },
          {
            "type": "html",
            "text": "<hr>",
            "id": "f4af353307c64085ce8ee2c13e14248d"
          },
          {
            "type": "html",
            "text": "<b>References</b>",
            "id": "e34cac796c51eb44315e07e0cd5aab63"
          },
          {
            "type": "html",
            "text": "\nA sample robots.txt (for the Portland Pattern Repository):",
            "id": "517b73c68c356de508a72bdfd98511e1"
          },
          {
            "type": "code",
            "text": " # Prevent all robots from getting lost in my databases\n User-agent: *\n Disallow: /cgi/\n Disallow: /cgi-bin/",
            "id": "6f586732df91d1c8098677293bf8c125"
          },
          {
            "type": "html",
            "text": "[http://usemod.com/cgi-bin/mb.pl?RobotsDotTxt usemod.com] :<br>A useful link to learn more about preventing a wiki site from being indexed, with \"sublinks\" worth following. ",
            "id": "645246c7b6a890e61f6454d138857dac"
          },
          {
            "type": "html",
            "text": "<hr>",
            "id": "f4af353307c64085ce8ee2c13e14248d"
          },
          {
            "type": "html",
            "text": "<b>Comments</b>",
            "id": "e52fad802d82b9490f65faa3323b671c"
          },
          {
            "type": "html",
            "text": "\nSure, why not? [[Meatball Wiki]] is indexed by Google. Some searches are bizarre. We got one trolling for kiddie porn once. Recently, Cliff decided to exclude bots from hitting the edit pages, but they are greatly encouraged to index the whole site there. On the other hand, Wiki is not indexed, nor will it likely ever be directly, although it is through the mirrors that aren't blocked. -- [[Sunir Shah]]",
            "id": "b86cf4658d68968651a4c51e966ba3a1"
          },
          {
            "type": "html",
            "text": "<hr>",
            "id": "f4af353307c64085ce8ee2c13e14248d"
          },
          {
            "type": "html",
            "text": "\nI wrote up a document about how to give the wiki pages some \"fake\" URLs that end in .html.  This doc is at [http://www.riceball.com/wikiwiki/MakingWikiWorkWithSearchEngines.html www.riceball.com] -- [[John Kawakami]]",
            "id": "c0928e47a3f8ac31e668f75319374b11"
          },
          {
            "type": "html",
            "text": "<hr>",
            "id": "f4af353307c64085ce8ee2c13e14248d"
          },
          {
            "type": "html",
            "text": "\nAug 6, 2004: Google no longer indexes the [[Edit Page]] or [[Edit Copy]] links. The wiki software now emits",
            "id": "8b75416eaefec6b76910c9c3052929a5"
          },
          {
            "type": "code",
            "text": "    <META NAME=\"ROBOTS\" CONTENT=\"NOINDEX, NOFOLLOW\">",
            "id": "2e35d6d6cd9744ba97cbd1650d4c038a"
          },
          {
            "type": "html",
            "text": "in the HTML header of the Edit page and the Edit Copy page. Google, like all properly-written robots, ignore such pages. See [[Wiki Spam]] for details.",
            "id": "8bf59819f93003ca8170c3d0eb616585"
          },
          {
            "type": "html",
            "text": "\nFeb 2. 2005: But, using the above implementation, the spider must still access the [[Edit Page]] or [[Edit Copy]] links to then find out they aren't supposed to be indexed.  Now google and blogs are implementing a rel=\"nofollow\" tag in links.  This could be added to the [[Edit Page]] button on each page to give robots a heads-up to not even bother trying to access the [[Edit Page]].  What is still unclear is whether the rel=\"nofollow\" tag really means \"do not follow this link\" to robots, or whether it means \"follow this link, but don't give it any credit in your page ranking scheme\".",
            "id": "d186f4eb01eb83b8374cccadbdf82a48"
          },
          {
            "type": "html",
            "text": "<hr>",
            "id": "f4af353307c64085ce8ee2c13e14248d"
          },
          {
            "type": "html",
            "text": "---\nMay 02, 2005: Meta Tag Tutorial Available \n[http://www.rahsoftware.com/tutorials/meta_tag_tutorial.aspx www.rahsoftware.com]",
            "id": "1185ab76e3dd67a8b0025ec70988e90e"
          },
          {
            "type": "html",
            "text": "\n[[Category Wiki Engine Review]]",
            "id": "161795e5e1cfed737326519554e5c46a"
          },
          {
            "type": "html",
            "text": "See original on  [http://c2.com/cgi/wiki?CanWebSearchEnginesIndexWikis c2.com]",
            "id": "d6fbfbf7394309b9a73d81ba3a62f616"
          }
        ]
      }
    },
    {
      "type": "fork",
      "site": "john.sfw.c2.com",
      "date": 1450360903363
    },
    {
      "type": "edit",
      "id": "a23b06a31f930a1aef546ffeccca44a1",
      "item": {
        "type": "html",
        "text": "However, the [[Robots.txt]] file should be properly configured.",
        "id": "a23b06a31f930a1aef546ffeccca44a1"
      },
      "date": 1450361023584
    },
    {
      "type": "fork",
      "site": "future.fedwiki.org",
      "date": 1503059819262
    }
  ]
}