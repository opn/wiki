{
  "title": "Adobe Voco",
  "journal": [
    {
      "type": "create",
      "item": {
        "story": "",
        "title": "Adobe Voco"
      },
      "date": 1501322484638,
      "source": {
        "pageTitle": "Adobe Voco",
        "pageSlug": "Adobe_Voco",
        "url": "https://en.wikipedia.org/wiki/Adobe_Voco",
        "date": 1501322484638,
        "link_dictionary": {
          "wiki_links": {
            "Technical details": {
              "section_number": 1,
              "section_level": 2,
              "dot_number": 1
            },
            "References": {
              "section_number": 2,
              "section_level": 2,
              "dot_number": 2
            }
          },
          "default": {
            "transport": "https://livecode.world/mediawiki/importSection"
          }
        },
        "transport": "https://livecode.world/mediawiki/transportAgain",
        "rev": 792087443,
        "repo": "https://github.com/LiveCodeWorld/lib_MediaWiki"
      }
    },
    {
      "type": "fork",
      "date": 1501322486704
    },
    {
      "type": "add",
      "item": {
        "type": "paragraph",
        "id": "4da69331-5906-4692-9e85-ac33c77badf8",
        "text": "Today, two new technological tricks that together couldinvade our past selvesand rewrite the rules of credibility. Also,we release something terrible intothe world - [http://www.radiolab.org/story/breaking-news/?utm_source=sharedUrl&utm_medium=metatag&utm_campaign=sharedUrl radiolab.org]"
      },
      "after": "5dd971b4-59c3-41a3-a7ba-1bcd140590bc",
      "id": "4da69331-5906-4692-9e85-ac33c77badf8",
      "date": 1501322495866
    },
    {
      "type": "add",
      "item": {
        "type": "html",
        "id": "a8b02488-2a6b-4372-a761-8739c2af448d",
        "text": "<img src=\"https://media2.wnyc.org/i/1200/627/l/80/1/ObamaAdler2.png\" width=100%>"
      },
      "after": "4da69331-5906-4692-9e85-ac33c77badf8",
      "id": "a8b02488-2a6b-4372-a761-8739c2af448d",
      "date": 1501322500853
    },
    {
      "type": "add",
      "item": {
        "type": "audio",
        "id": "72fb854c-5e7f-412c-a50d-c0d7a10d45d9",
        "text": "https://www.podtrac.com/pts/redirect.mp3/audio.wnyc.org/radiolab_podcast/radiolab_podcast17breakingnews.mp3\nBreaking News - [http://www.radiolab.org/story/breaking-news/?utm_source=sharedUrl&utm_medium=metatag&utm_campaign=sharedUrl radiolab.org]"
      },
      "after": "a8b02488-2a6b-4372-a761-8739c2af448d",
      "id": "72fb854c-5e7f-412c-a50d-c0d7a10d45d9",
      "date": 1501322503528
    },
    {
      "item": {
        "type": "factory",
        "id": "7dc615aa10d65cba"
      },
      "id": "7dc615aa10d65cba",
      "type": "add",
      "after": "98a45451-3738-491d-be04-69c1c0581f64",
      "date": 1501322748185
    },
    {
      "type": "move",
      "order": [
        "5dd971b4-59c3-41a3-a7ba-1bcd140590bc",
        "4da69331-5906-4692-9e85-ac33c77badf8",
        "a8b02488-2a6b-4372-a761-8739c2af448d",
        "72fb854c-5e7f-412c-a50d-c0d7a10d45d9",
        "7dc615aa10d65cba",
        "98a45451-3738-491d-be04-69c1c0581f64"
      ],
      "id": "7dc615aa10d65cba",
      "date": 1501322749779
    },
    {
      "type": "edit",
      "id": "7dc615aa10d65cba",
      "item": {
        "type": "paragraph",
        "id": "7dc615aa10d65cba",
        "text": "Special thanks to everyone on the University of Southern California team who helped out with the facial manipulation: Kyle Olszewski, Koki Nagano, Ronald Yu, Yi Zhou, Jaewoo Seo, Shunsuke Saito, and Hao Li. Check out more of their work here.\n\n"
      },
      "date": 1501322751703
    },
    {
      "item": {
        "type": "factory",
        "id": "5a6f5e2bba672ff6"
      },
      "id": "5a6f5e2bba672ff6",
      "type": "add",
      "after": "98a45451-3738-491d-be04-69c1c0581f64",
      "date": 1501323411131
    },
    {
      "type": "move",
      "order": [
        "5dd971b4-59c3-41a3-a7ba-1bcd140590bc",
        "4da69331-5906-4692-9e85-ac33c77badf8",
        "a8b02488-2a6b-4372-a761-8739c2af448d",
        "72fb854c-5e7f-412c-a50d-c0d7a10d45d9",
        "7dc615aa10d65cba",
        "5a6f5e2bba672ff6",
        "98a45451-3738-491d-be04-69c1c0581f64"
      ],
      "id": "5a6f5e2bba672ff6",
      "date": 1501323413593
    },
    {
      "type": "edit",
      "id": "5a6f5e2bba672ff6",
      "item": {
        "type": "video",
        "id": "5a6f5e2bba672ff6",
        "text": "YOUTUBE undefined\n(double-click to edit caption)\n"
      },
      "date": 1501323427271
    },
    {
      "type": "remove",
      "id": "5a6f5e2bba672ff6",
      "date": 1501323439931
    },
    {
      "type": "edit",
      "id": "98a45451-3738-491d-be04-69c1c0581f64",
      "item": {
        "type": "markdown",
        "id": "98a45451-3738-491d-be04-69c1c0581f64",
        "text": "# See also\n* [[Technical details]]\n- [[https://www.youtube.com/embed/vprETB4dzNE?showinfo=0&rel=0 youtube]]"
      },
      "date": 1501323476571
    },
    {
      "type": "edit",
      "id": "98a45451-3738-491d-be04-69c1c0581f64",
      "item": {
        "type": "markdown",
        "id": "98a45451-3738-491d-be04-69c1c0581f64",
        "text": "# See also\n* [[Technical details]]\n- [[https://www.youtube.com/embed/vprETB4dzNE?showinfo=0&rel=0 youtube]\n- [http://futureoffakenews.com/ futureoffakenews.com]"
      },
      "date": 1501323525230
    },
    {
      "type": "edit",
      "id": "98a45451-3738-491d-be04-69c1c0581f64",
      "item": {
        "type": "markdown",
        "id": "98a45451-3738-491d-be04-69c1c0581f64",
        "text": "# See also\n* [[Technical details]]\n- [https://www.youtube.com/embed/vprETB4dzNE?showinfo=0&rel=0 youtube]\n- [http://futureoffakenews.com/ futureoffakenews.com]"
      },
      "date": 1501323531837
    },
    {
      "item": {
        "type": "factory",
        "id": "19b0da5cf09b4308"
      },
      "id": "19b0da5cf09b4308",
      "type": "add",
      "after": "98a45451-3738-491d-be04-69c1c0581f64",
      "date": 1501325348834
    },
    {
      "type": "move",
      "order": [
        "5dd971b4-59c3-41a3-a7ba-1bcd140590bc",
        "19b0da5cf09b4308",
        "4da69331-5906-4692-9e85-ac33c77badf8",
        "a8b02488-2a6b-4372-a761-8739c2af448d",
        "72fb854c-5e7f-412c-a50d-c0d7a10d45d9",
        "7dc615aa10d65cba",
        "98a45451-3738-491d-be04-69c1c0581f64"
      ],
      "id": "19b0da5cf09b4308",
      "date": 1501325351169
    },
    {
      "type": "edit",
      "id": "19b0da5cf09b4308",
      "item": {
        "type": "video",
        "id": "19b0da5cf09b4308",
        "text": "YOUTUBE RB7upq8nzIU\n(double-click to edit caption)\n"
      },
      "date": 1501325360811
    },
    {
      "type": "edit",
      "id": "19b0da5cf09b4308",
      "item": {
        "type": "video",
        "id": "19b0da5cf09b4308",
        "text": "YOUTUBE RB7upq8nzIU\nThis video presents a research project called VoCo -- a collaboration between Princeton University and Adobe Research. Our goal is to make it easy to edit audio narrations by typing - [sfad dsfa]"
      },
      "date": 1501325392039
    },
    {
      "type": "edit",
      "id": "19b0da5cf09b4308",
      "item": {
        "type": "video",
        "id": "19b0da5cf09b4308",
        "text": "YOUTUBE RB7upq8nzIU\nThis video presents a research project called VoCo -- a collaboration between Princeton University and Adobe Research. Our goal is to make it easy to edit audio narrations by typing - [http://gfx.cs.princeton.edu/pubs/Jin_2017_VTI/ princeton.edu]"
      },
      "date": 1501325410329
    },
    {
      "item": {
        "type": "factory",
        "id": "12246326d7cfea15"
      },
      "id": "12246326d7cfea15",
      "type": "add",
      "after": "98a45451-3738-491d-be04-69c1c0581f64",
      "date": 1501325493857
    },
    {
      "type": "move",
      "order": [
        "5dd971b4-59c3-41a3-a7ba-1bcd140590bc",
        "19b0da5cf09b4308",
        "4da69331-5906-4692-9e85-ac33c77badf8",
        "a8b02488-2a6b-4372-a761-8739c2af448d",
        "72fb854c-5e7f-412c-a50d-c0d7a10d45d9",
        "7dc615aa10d65cba",
        "12246326d7cfea15",
        "98a45451-3738-491d-be04-69c1c0581f64"
      ],
      "id": "12246326d7cfea15",
      "date": 1501325495212
    },
    {
      "type": "edit",
      "id": "12246326d7cfea15",
      "item": {
        "type": "video",
        "id": "12246326d7cfea15",
        "text": "YOUTUBE 9Yq67CjDqvw\n(double-click to edit caption)\n"
      },
      "date": 1501325502966
    },
    {
      "type": "edit",
      "id": "12246326d7cfea15",
      "item": {
        "type": "video",
        "id": "12246326d7cfea15",
        "text": "YOUTUBE 9Yq67CjDqvw\nSynthesizing Obama: Learning Lip Sync from Audio\nSupasorn Suwajanakorn, Steven M. Seitz, Ira Kemelmacher-Shlizerman\nSIGGRAPH 2017"
      },
      "date": 1501325544649
    },
    {
      "item": {
        "type": "factory",
        "id": "4a6441f2abed2623"
      },
      "id": "4a6441f2abed2623",
      "type": "add",
      "after": "98a45451-3738-491d-be04-69c1c0581f64",
      "date": 1501325554783
    },
    {
      "type": "move",
      "order": [
        "5dd971b4-59c3-41a3-a7ba-1bcd140590bc",
        "19b0da5cf09b4308",
        "4da69331-5906-4692-9e85-ac33c77badf8",
        "a8b02488-2a6b-4372-a761-8739c2af448d",
        "72fb854c-5e7f-412c-a50d-c0d7a10d45d9",
        "7dc615aa10d65cba",
        "12246326d7cfea15",
        "4a6441f2abed2623",
        "98a45451-3738-491d-be04-69c1c0581f64"
      ],
      "id": "4a6441f2abed2623",
      "date": 1501325556098
    },
    {
      "type": "edit",
      "id": "4a6441f2abed2623",
      "item": {
        "type": "paragraph",
        "id": "4a6441f2abed2623",
        "text": "Given audio of President Barack Obama, we synthesize a high quality video of him speaking with accurate lip sync, composited into a target video clip. Trained on many hours of his weekly address footage, a recurrent neural network learns the mapping from raw audio features to mouth shapes. Given the mouth shape at each time instant, we synthesize high quality mouth texture, and composite it with proper 3D pose matching to change what he appears to be saying in a target video to match the input audio track\n"
      },
      "date": 1501325558050
    },
    {
      "type": "edit",
      "id": "12246326d7cfea15",
      "item": {
        "type": "video",
        "id": "12246326d7cfea15",
        "text": "YOUTUBE 9Yq67CjDqvw\nads"
      },
      "date": 1501325567147
    },
    {
      "type": "edit",
      "id": "12246326d7cfea15",
      "item": {
        "type": "video",
        "id": "12246326d7cfea15",
        "text": "YOUTUBE 9Yq67CjDqvw\nSynthesizing Obama: Learning Lip Sync from Audio\nSupasorn Suwajanakorn, Steven M. Seitz, Ira Kemelmacher-Shlizerman - SIGGRAPH 2017"
      },
      "date": 1501325585955
    },
    {
      "item": {
        "type": "factory",
        "id": "590fb833aaa0070f"
      },
      "id": "590fb833aaa0070f",
      "type": "add",
      "after": "98a45451-3738-491d-be04-69c1c0581f64",
      "date": 1501325628873
    },
    {
      "type": "move",
      "order": [
        "5dd971b4-59c3-41a3-a7ba-1bcd140590bc",
        "19b0da5cf09b4308",
        "4da69331-5906-4692-9e85-ac33c77badf8",
        "a8b02488-2a6b-4372-a761-8739c2af448d",
        "72fb854c-5e7f-412c-a50d-c0d7a10d45d9",
        "7dc615aa10d65cba",
        "12246326d7cfea15",
        "4a6441f2abed2623",
        "590fb833aaa0070f",
        "98a45451-3738-491d-be04-69c1c0581f64"
      ],
      "id": "590fb833aaa0070f",
      "date": 1501325630772
    },
    {
      "type": "edit",
      "id": "590fb833aaa0070f",
      "item": {
        "type": "video",
        "id": "590fb833aaa0070f",
        "text": "YOUTUBE ohmajJTcpNk\n(double-click to edit caption)\n"
      },
      "date": 1501325641880
    },
    {
      "type": "edit",
      "id": "590fb833aaa0070f",
      "item": {
        "type": "video",
        "id": "590fb833aaa0070f",
        "text": "YOUTUBE ohmajJTcpNk\nWe present a novel approach for real-time facial reenactment of a monocular target video sequence (e.g., Youtube video). The source sequence is also a monocular video stream, captured live with a commodity webcam. Our goal is to animate the facial expressions of the target video by a source actor and re-render the manipulated output video in a photo-realistic fashion. To this end, we first address the under-constrained problem of facial identity recovery from monocular video by non-rigid model-based bundling. At run time, we track facial expressions of both source and target video using a dense photometric consistency measure. Reenactment is then achieved by fast and efficient deformation transfer between source and target. The mouth interior that best matches the re-targeted expression is retrieved from the target sequence and warped to produce an accurate fit. Finally, we convincingly re-render the synthesized target face on top of the corresponding video stream such that it seamlessly blends with the real-world illumination. We demonstrate our method in a live setup, where Youtube videos are reenacted in real time."
      },
      "date": 1501325662318
    },
    {
      "type": "edit",
      "id": "4a6441f2abed2623",
      "item": {
        "type": "paragraph",
        "id": "4a6441f2abed2623",
        "text": "Given audio of President Barack Obama, we synthesize a high quality video of him speaking with accurate lip sync, composited into a target video clip. Trained on many hours of his weekly address footage, a recurrent neural network learns the mapping from raw audio features to mouth shapes. Given the mouth shape at each time instant, we synthesize high quality mouth texture, and composite it with proper 3D pose matching to change what he appears to be saying in a target video to match the input audio track"
      },
      "date": 1501325669845
    },
    {
      "type": "add",
      "id": "33bacd84ec724598",
      "item": {
        "type": "paragraph",
        "id": "33bacd84ec724598",
        "text": "# Face2Face"
      },
      "after": "4a6441f2abed2623",
      "date": 1501325679550
    },
    {
      "type": "add",
      "id": "30ef6df4497f7c7c",
      "item": {
        "type": "paragraph",
        "id": "30ef6df4497f7c7c",
        "text": "We present a novel approach for real-time facial reenactment of a monocular target video sequence (e.g., Youtube video). "
      },
      "after": "33bacd84ec724598",
      "date": 1501325689678
    },
    {
      "type": "add",
      "id": "bb9e03aa4c78e6fb",
      "item": {
        "type": "paragraph",
        "id": "bb9e03aa4c78e6fb",
        "text": "The source sequence is also a monocular video stream, captured live with a commodity webcam. Our goal is to animate the facial expressions of the target video by a source actor and re-render the manipulated output video in a photo-realistic fashion. "
      },
      "after": "30ef6df4497f7c7c",
      "date": 1501325692529
    },
    {
      "type": "add",
      "id": "225bcaa8a4f46d53",
      "item": {
        "type": "paragraph",
        "id": "225bcaa8a4f46d53",
        "text": "To this end, we first address the under-constrained problem of facial identity recovery from monocular video by non-rigid model-based bundling. At run time, we track facial expressions of both source and target video using a dense photometric consistency measure. Reenactment is then achieved by fast and efficient deformation transfer between source and target. "
      },
      "after": "bb9e03aa4c78e6fb",
      "date": 1501325698536
    },
    {
      "type": "add",
      "id": "8f33059a3fb61091",
      "item": {
        "type": "paragraph",
        "id": "8f33059a3fb61091",
        "text": "The mouth interior that best matches the re-targeted expression is retrieved from the target sequence and warped to produce an accurate fit. Finally, we convincingly re-render the synthesized target face on top of the corresponding video stream such that it seamlessly blends with the real-world illumination. We demonstrate our method in a live setup, where Youtube videos are reenacted in real time."
      },
      "after": "225bcaa8a4f46d53",
      "date": 1501325701182
    },
    {
      "type": "edit",
      "id": "590fb833aaa0070f",
      "item": {
        "type": "video",
        "id": "590fb833aaa0070f",
        "text": "YOUTUBE ohmajJTcpNk\nSynthesizing Obama: Learning Lip Sync from Audio"
      },
      "date": 1501325724263
    },
    {
      "type": "move",
      "order": [
        "5dd971b4-59c3-41a3-a7ba-1bcd140590bc",
        "19b0da5cf09b4308",
        "4da69331-5906-4692-9e85-ac33c77badf8",
        "a8b02488-2a6b-4372-a761-8739c2af448d",
        "72fb854c-5e7f-412c-a50d-c0d7a10d45d9",
        "7dc615aa10d65cba",
        "12246326d7cfea15",
        "4a6441f2abed2623",
        "33bacd84ec724598",
        "30ef6df4497f7c7c",
        "bb9e03aa4c78e6fb",
        "590fb833aaa0070f",
        "225bcaa8a4f46d53",
        "8f33059a3fb61091",
        "98a45451-3738-491d-be04-69c1c0581f64"
      ],
      "id": "590fb833aaa0070f",
      "date": 1501325728667
    },
    {
      "type": "edit",
      "id": "33bacd84ec724598",
      "item": {
        "type": "markdown",
        "id": "33bacd84ec724598",
        "text": "# Face2Face"
      },
      "date": 1501325729925
    },
    {
      "type": "edit",
      "id": "4a6441f2abed2623",
      "item": {
        "type": "paragraph",
        "id": "4a6441f2abed2623",
        "text": "Given audio of President Barack Obama, we synthesize a high quality video of him speaking with accurate lip sync, composited into a target video clip. Trained on many hours of his weekly address footage, a recurrent neural network learns the mapping from raw audio features to mouth shapes. "
      },
      "date": 1501325756456
    },
    {
      "type": "add",
      "id": "f57dd74c3939c736",
      "item": {
        "type": "paragraph",
        "id": "f57dd74c3939c736",
        "text": "Given the mouth shape at each time instant, we synthesize high quality mouth texture, and composite it with proper 3D pose matching to change what he appears to be saying in a target video to match the input audio track"
      },
      "after": "4a6441f2abed2623",
      "date": 1501325757815
    },
    {
      "type": "move",
      "order": [
        "5dd971b4-59c3-41a3-a7ba-1bcd140590bc",
        "19b0da5cf09b4308",
        "4da69331-5906-4692-9e85-ac33c77badf8",
        "a8b02488-2a6b-4372-a761-8739c2af448d",
        "72fb854c-5e7f-412c-a50d-c0d7a10d45d9",
        "7dc615aa10d65cba",
        "4a6441f2abed2623",
        "12246326d7cfea15",
        "f57dd74c3939c736",
        "33bacd84ec724598",
        "30ef6df4497f7c7c",
        "bb9e03aa4c78e6fb",
        "590fb833aaa0070f",
        "225bcaa8a4f46d53",
        "8f33059a3fb61091",
        "98a45451-3738-491d-be04-69c1c0581f64"
      ],
      "id": "4a6441f2abed2623",
      "date": 1501325759824
    },
    {
      "type": "edit",
      "id": "4a6441f2abed2623",
      "item": {
        "type": "paragraph",
        "id": "4a6441f2abed2623",
        "text": "# Learing Lip Sync"
      },
      "date": 1501325772401
    },
    {
      "type": "edit",
      "id": "4a6441f2abed2623",
      "item": {
        "type": "markdown",
        "id": "4a6441f2abed2623",
        "text": "# Learing Lip Sync"
      },
      "date": 1501325774224
    },
    {
      "type": "add",
      "id": "20c7d43f07de34ff",
      "item": {
        "type": "paragraph",
        "id": "20c7d43f07de34ff",
        "text": "Given audio of President Barack Obama, we synthesize a high quality video of him speaking with accurate lip sync, composited into a target video clip. Trained on many hours of his weekly address footage, a recurrent neural network learns the mapping from raw audio features to mouth shapes. "
      },
      "after": "4a6441f2abed2623",
      "date": 1501325775216
    },
    {
      "type": "edit",
      "id": "33bacd84ec724598",
      "item": {
        "type": "markdown",
        "id": "33bacd84ec724598",
        "text": "# [[Face2Face]]"
      },
      "date": 1501356292884
    },
    {
      "type": "remove",
      "id": "30ef6df4497f7c7c",
      "date": 1501356297621
    },
    {
      "type": "remove",
      "id": "bb9e03aa4c78e6fb",
      "date": 1501356299885
    },
    {
      "type": "remove",
      "id": "590fb833aaa0070f",
      "date": 1501356306638
    },
    {
      "type": "remove",
      "id": "225bcaa8a4f46d53",
      "date": 1501356314455
    },
    {
      "type": "remove",
      "id": "8f33059a3fb61091",
      "date": 1501356316845
    },
    {
      "type": "remove",
      "id": "33bacd84ec724598",
      "date": 1501356321812
    },
    {
      "type": "edit",
      "id": "98a45451-3738-491d-be04-69c1c0581f64",
      "item": {
        "type": "markdown",
        "id": "98a45451-3738-491d-be04-69c1c0581f64",
        "text": "# See also\n* [[Technical details]]\n- [https://www.youtube.com/embed/vprETB4dzNE?showinfo=0&rel=0 youtube]\n- [http://futureoffakenews.com/ futureoffakenews.com]\n- [[Face2Face]]"
      },
      "date": 1501356330925
    },
    {
      "type": "edit",
      "id": "4a6441f2abed2623",
      "item": {
        "type": "markdown",
        "id": "4a6441f2abed2623",
        "text": "# [[Learning Lip Sync]]"
      },
      "date": 1501356345765
    },
    {
      "type": "remove",
      "id": "20c7d43f07de34ff",
      "date": 1501356356038
    },
    {
      "type": "remove",
      "id": "12246326d7cfea15",
      "date": 1501356358726
    },
    {
      "type": "remove",
      "id": "7dc615aa10d65cba",
      "date": 1501356360772
    },
    {
      "type": "remove",
      "id": "f57dd74c3939c736",
      "date": 1501356414732
    },
    {
      "type": "add",
      "item": {
        "type": "paragraph",
        "id": "7dc615aa10d65cba",
        "text": "Special thanks to everyone on the University of Southern California team who helped out with the facial manipulation: Kyle Olszewski, Koki Nagano, Ronald Yu, Yi Zhou, Jaewoo Seo, Shunsuke Saito, and Hao Li. Check out more of their work here.\n\n"
      },
      "after": "72fb854c-5e7f-412c-a50d-c0d7a10d45d9",
      "id": "7dc615aa10d65cba",
      "date": 1501356422974
    },
    {
      "type": "remove",
      "id": "4a6441f2abed2623",
      "date": 1501356439023
    },
    {
      "type": "add",
      "item": {
        "type": "markdown",
        "id": "6101e5ff6aa9186a",
        "text": "# See also\n\n- [https://www.youtube.com/embed/vprETB4dzNE?showinfo=0&rel=0 youtube]\n- [http://futureoffakenews.com/ futureoffakenews.com]\n- [[Face2Face]]\n- [[Adobe Voco]]\n- [[Learning Lip Sync]]",
        "alias": "98a45451-3738-491d-be04-69c1c0581f64"
      },
      "after": "7dc615aa10d65cba",
      "id": "6101e5ff6aa9186a",
      "date": 1501356448200
    },
    {
      "type": "edit",
      "id": "98a45451-3738-491d-be04-69c1c0581f64",
      "item": {
        "type": "markdown",
        "id": "98a45451-3738-491d-be04-69c1c0581f64",
        "text": "# See also\n\n- [https://www.youtube.com/embed/vprETB4dzNE?showinfo=0&rel=0 youtube]\n- [http://futureoffakenews.com/ futureoffakenews.com]\n- [[Face2Face]]\n- [[Learning Lip Sync]]"
      },
      "date": 1501356453305
    },
    {
      "type": "add",
      "item": {
        "type": "paragraph",
        "id": "acdef076bb08f298",
        "text": "[[Adobe Voco]] is a new experimental software from Adobe billed as [[Photoshop for Sound]]."
      },
      "id": "acdef076bb08f298",
      "date": 1501356514726
    },
    {
      "type": "edit",
      "id": "98a45451-3738-491d-be04-69c1c0581f64",
      "item": {
        "type": "markdown",
        "id": "98a45451-3738-491d-be04-69c1c0581f64",
        "text": "# See also\n\n- [https://www.youtube.com/embed/vprETB4dzNE?showinfo=0&rel=0 youtube]\n- [http://futureoffakenews.com/ futureoffakenews.com]\n- [[Face2Face]]\n- [[Learning Lip Sync]]\n- [[3D Installation Research]]\n- [[3D Sound Libraries]]\n- [[Ableton 3D Sound]]\n- [[Unity 3D]]\n- [[Ambisonics]]\n- [[Basic Installation]]\n- [[Sound and Music]]\n- [[Thought Garden Installation]]\n- [[Sound art]]\n- [[Adobe Voco]]\n- [[Free Music Archive]]"
      },
      "date": 1501842058240
    },
    {
      "type": "remove",
      "id": "6101e5ff6aa9186a",
      "date": 1501842078704
    },
    {
      "type": "fork",
      "site": "space.thought.garden",
      "date": 1502030005513
    }
  ],
  "story": [
    {
      "type": "paragraph",
      "id": "acdef076bb08f298",
      "text": "[[Adobe Voco]] is a new experimental software from Adobe billed as [[Photoshop for Sound]]."
    },
    {
      "type": "paragraph",
      "id": "5dd971b4-59c3-41a3-a7ba-1bcd140590bc",
      "text": "Adobe Voco is an audio editing and generating prototype software by Adobe that enables novel editing and generation of audio. It has been popularly dubbed \"the Photoshop of voice\". It was first previewed at the Adobe MAX event in November 2016 and is expected to be part of the Adobe Creative Suite - [https://en.wikipedia.org/wiki/Adobe_Voco wikipedia]"
    },
    {
      "type": "video",
      "id": "19b0da5cf09b4308",
      "text": "YOUTUBE RB7upq8nzIU\nThis video presents a research project called VoCo -- a collaboration between Princeton University and Adobe Research. Our goal is to make it easy to edit audio narrations by typing - [http://gfx.cs.princeton.edu/pubs/Jin_2017_VTI/ princeton.edu]"
    },
    {
      "type": "paragraph",
      "id": "4da69331-5906-4692-9e85-ac33c77badf8",
      "text": "Today, two new technological tricks that together couldinvade our past selvesand rewrite the rules of credibility. Also,we release something terrible intothe world - [http://www.radiolab.org/story/breaking-news/?utm_source=sharedUrl&utm_medium=metatag&utm_campaign=sharedUrl radiolab.org]"
    },
    {
      "type": "html",
      "id": "a8b02488-2a6b-4372-a761-8739c2af448d",
      "text": "<img src=\"https://media2.wnyc.org/i/1200/627/l/80/1/ObamaAdler2.png\" width=100%>"
    },
    {
      "type": "audio",
      "id": "72fb854c-5e7f-412c-a50d-c0d7a10d45d9",
      "text": "https://www.podtrac.com/pts/redirect.mp3/audio.wnyc.org/radiolab_podcast/radiolab_podcast17breakingnews.mp3\nBreaking News - [http://www.radiolab.org/story/breaking-news/?utm_source=sharedUrl&utm_medium=metatag&utm_campaign=sharedUrl radiolab.org]"
    },
    {
      "type": "paragraph",
      "id": "7dc615aa10d65cba",
      "text": "Special thanks to everyone on the University of Southern California team who helped out with the facial manipulation: Kyle Olszewski, Koki Nagano, Ronald Yu, Yi Zhou, Jaewoo Seo, Shunsuke Saito, and Hao Li. Check out more of their work here.\n\n"
    },
    {
      "type": "markdown",
      "id": "98a45451-3738-491d-be04-69c1c0581f64",
      "text": "# See also\n\n- [https://www.youtube.com/embed/vprETB4dzNE?showinfo=0&rel=0 youtube]\n- [http://futureoffakenews.com/ futureoffakenews.com]\n- [[Face2Face]]\n- [[Learning Lip Sync]]\n- [[3D Installation Research]]\n- [[3D Sound Libraries]]\n- [[Ableton 3D Sound]]\n- [[Unity 3D]]\n- [[Ambisonics]]\n- [[Basic Installation]]\n- [[Sound and Music]]\n- [[Thought Garden Installation]]\n- [[Sound art]]\n- [[Adobe Voco]]\n- [[Free Music Archive]]"
    }
  ]
}