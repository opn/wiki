{
  "title": "Google Site Maps",
  "story": [
    {
      "type": "paragraph",
      "id": "1ef93e59022459c7",
      "text": "[[Sitemap]]s are an easy way for webmasters to inform search engines about pages on their sites that are available for crawling - [http://www.sitemaps.org/ sitemaps.org]"
    },
    {
      "type": "paragraph",
      "id": "62a4985ed8b11cf1",
      "text": "In its simplest form, a [[Sitemap]] is an [[XML]] file that lists URLs for a site along with additional metadata about each URL (when it was last updated, how often it usually changes, and how important it is, relative to other URLs in the site) so that search engines can more intelligently crawl the site."
    },
    {
      "type": "paragraph",
      "id": "a7d7038a7a92342d",
      "text": "Web crawlers usually discover pages from links within the site and from other sites. Sitemaps supplement this data to allow crawlers that support Sitemaps to pick up all URLs in the Sitemap and learn about those URLs using the associated metadata. Using the Sitemap protocol does not guarantee that web pages are included in search engines, but provides hints for web crawlers to do a better job of crawling your site."
    },
    {
      "type": "reference",
      "id": "1908f52073832a4d",
      "site": "c0de.academy",
      "slug": "robotstxt",
      "title": "robots.txt",
      "text": "The robots exclusion standard, also known as the robots exclusion protocol or simply robots.txt, is a standard used by websites to communicate with [[Web Crawlers]] and other web robots - [https://en.wikipedia.org/wiki/Robots_exclusion_standard wikipedia]"
    }
  ],
  "journal": [
    {
      "type": "create",
      "item": {
        "title": "Google Site Maps",
        "story": []
      },
      "date": 1448307256471
    },
    {
      "item": {
        "type": "factory",
        "id": "1ef93e59022459c7"
      },
      "id": "1ef93e59022459c7",
      "type": "add",
      "date": 1448307257826
    },
    {
      "type": "edit",
      "id": "1ef93e59022459c7",
      "item": {
        "type": "paragraph",
        "id": "1ef93e59022459c7",
        "text": "Sitemaps are an easy way for webmasters to inform search engines about pages on their sites that are available for crawling. In its simplest form, a Sitemap is an XML file that lists URLs for a site along with additional metadata about each URL (when it was last updated, how often it usually changes, and how important it is, relative to other URLs in the site) so that search engines can more intelligently crawl the site."
      },
      "date": 1448739509427
    },
    {
      "type": "add",
      "id": "a7d7038a7a92342d",
      "item": {
        "type": "paragraph",
        "id": "a7d7038a7a92342d",
        "text": "Web crawlers usually discover pages from links within the site and from other sites. Sitemaps supplement this data to allow crawlers that support Sitemaps to pick up all URLs in the Sitemap and learn about those URLs using the associated metadata. Using the Sitemap protocol does not guarantee that web pages are included in search engines, but provides hints for web crawlers to do a better job of crawling your site."
      },
      "after": "1ef93e59022459c7",
      "date": 1448739510860
    },
    {
      "type": "edit",
      "id": "1ef93e59022459c7",
      "item": {
        "type": "paragraph",
        "id": "1ef93e59022459c7",
        "text": "[[Sitemaps]] are an easy way for webmasters to inform search engines about pages on their sites that are available for crawling. "
      },
      "date": 1448739522440
    },
    {
      "type": "add",
      "id": "62a4985ed8b11cf1",
      "item": {
        "type": "paragraph",
        "id": "62a4985ed8b11cf1",
        "text": "In its simplest form, a [[Sitemap]] is an [[XML]] file that lists URLs for a site along with additional metadata about each URL (when it was last updated, how often it usually changes, and how important it is, relative to other URLs in the site) so that search engines can more intelligently crawl the site."
      },
      "after": "1ef93e59022459c7",
      "date": 1448739543050
    },
    {
      "type": "edit",
      "id": "1ef93e59022459c7",
      "item": {
        "type": "paragraph",
        "id": "1ef93e59022459c7",
        "text": "[[Sitemaps]] are an easy way for webmasters to inform search engines about pages on their sites that are available for crawling - [http://www.sitemaps.org/ sitemaps.org]"
      },
      "date": 1448739564020
    },
    {
      "type": "edit",
      "id": "1ef93e59022459c7",
      "item": {
        "type": "paragraph",
        "id": "1ef93e59022459c7",
        "text": "[[Sitemap]]s are an easy way for webmasters to inform search engines about pages on their sites that are available for crawling - [http://www.sitemaps.org/ sitemaps.org]"
      },
      "date": 1448739574971
    },
    {
      "type": "fork",
      "site": "c0de.academy",
      "date": 1448783374007
    },
    {
      "item": {
        "type": "factory",
        "id": "1908f52073832a4d"
      },
      "id": "1908f52073832a4d",
      "type": "add",
      "after": "a7d7038a7a92342d",
      "date": 1448784073646
    },
    {
      "type": "edit",
      "id": "1908f52073832a4d",
      "item": {
        "type": "reference",
        "id": "1908f52073832a4d",
        "site": "c0de.academy",
        "slug": "robotstxt",
        "title": "robots.txt",
        "text": "The robots exclusion standard, also known as the robots exclusion protocol or simply robots.txt, is a standard used by websites to communicate with [[Web Crawlers]] and other web robots - [https://en.wikipedia.org/wiki/Robots_exclusion_standard wikipedia]"
      },
      "date": 1448784080799
    }
  ]
}